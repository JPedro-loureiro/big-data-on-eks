apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: sync-iceberg-tpch-postgres-small
  labels:
    # The strimzi.io/cluster label identifies the KafkaConnect instance
    # in which to create this connector. That KafkaConnect instance
    # must have the strimzi.io/use-connector-resources annotation
    # set to true.
    strimzi.io/cluster: main-kafka-connect-cluster
spec:
  class: io.tabular.iceberg.connect.IcebergSinkConnector
  tasksMax: 2
  config:
    iceberg.catalog.catalog-impl: org.apache.iceberg.aws.glue.GlueCatalog
    iceberg.catalog.warehouse: "s3://datalake-bronze-590183863248"
    iceberg.catalog.io-impl: org.apache.iceberg.aws.s3.S3FileIO
    iceberg.tables.default-partition-by: "event_timestamp_hour"
    iceberg.tables.auto-create-enabled: true
    iceberg.tables.evolve-schema-enabled: true
    iceberg.control.commit.interval-ms: 300000
    iceberg.writer.batch-size-bytes: 10485760
    iceberg.writer.max-file-size-bytes: 134217728
    iceberg.writer.target-file-size-bytes: 134217728
    # Avro Converters + Schema Registry
    key.converter: io.confluent.connect.avro.AvroConverter
    key.converter.schema.registry.url: "http://schema-registry.kafka.svc.cluster.local:8081"
    key.converter.schemas.enable: "true"

    value.converter: io.confluent.connect.avro.AvroConverter
    value.converter.schema.registry.url: "http://schema-registry.kafka.svc.cluster.local:8081"
    value.converter.schemas.enable: "true"
    topics: >-
      tpch_postgres.public.customer,
      tpch_postgres.public.supplier,
      tpch_postgres.public.nation,
      tpch_postgres.public.region,
      tpch_postgres.public.partsupp,
      tpch_postgres.public.part
    iceberg.tables: >-
      datalake_bronze.customer,
      datalake_bronze.supplier,
      datalake_bronze.nation,
      datalake_bronze.region,
      datalake_bronze.partsupp,
      datalake_bronze.part